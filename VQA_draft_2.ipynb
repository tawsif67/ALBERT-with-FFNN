{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawsif67/ALBERT-with-FFNN/blob/main/VQA_draft_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CEYz9215-vH",
        "outputId": "8ebd4853-2ad6-4c1f-d20b-8ecd58edd31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qQK-4nIa_6SX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6df866f35e9545e49e621d8785ab9ca9",
            "a11bb38dcf424624bb76960ddb300d4d",
            "44a077e053e34f3e8a47d15c53699d0d",
            "ed5a46792ccb41d890363b4829df0353",
            "7cdb8fbc1092466ea35e6860eb49f189",
            "a64d95859e234c609f9257bd3c29d120",
            "5a2da52c76bf4244a8d78bce58036621",
            "a326818b45864539a980002e64bd2b37",
            "ceafacf167064863a507c6b77dbb7fc6",
            "dc930076859f4854bdd4041fb84794a5",
            "b5fe2341b6db4f24b7595ab2f8bafe67"
          ]
        },
        "outputId": "671d3188-2536-4468-d062-11486dfb6b0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6df866f35e9545e49e621d8785ab9ca9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import ViltConfig\n",
        "config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(config.label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcgRCUshsQGQ",
        "outputId": "11f8256b-9864-4a7f-d2ab-d67a31f4bab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 82, '000': 3001, '1': 12, '1 4': 1990, '1 foot': 335, '1 hour': 1967, '1 in back': 3026, '1 in front': 2299, '1 in middle': 3059, '1 inch': 2146, '1 on left': 1723, '1 on right': 314, '1 way': 201, '1 world': 2360, '1 year': 2807, '1.00': 3000, '10': 103, '10 feet': 941, '10 inches': 2929, '10 years': 47, '100': 1161, '100 feet': 150, '100 year party ct': 3120, '1000': 1511, '101': 2789, '106': 2450, '10:00': 1661, '10:05': 1184, '10:08': 3117, '10:10': 519, '10:15': 2766, '10:20': 2177, '10:25': 2048, '10:30': 2410, '10:35': 3103, '10:40': 2040, '10:45': 3060, '10:50': 2880, '10:55': 2451, '11': 191, '11:00': 1907, '11:05': 2730, '11:10': 2646, '11:15': 2020, '11:20': 2141, '11:25': 3062, '11:30': 995, '11:35': 2198, '11:45': 2580, '11:50': 2790, '11:55': 186, '12': 396, '12 feet': 450, '120': 1973, '12:00': 1763, '12:05': 1759, '12:10': 2036, '12:15': 754, '12:20': 2754, '12:25': 2035, '12:28': 2033, '12:30': 1073, '12:35': 1651, '12:40': 3073, '12:45': 2245, '12:50': 2506, '12:55': 196, '13': 164, '14': 383, '15': 418, '15 feet': 2105, '150': 1846, '16': 704, '17': 1159, '18': 668, '19': 1171, '193': 1604, '1950': 1349, '1950s': 258, '1980': 2925, '1990': 1259, '1:00': 1758, '1:05': 2734, '1:10': 3025, '1:15': 3008, '1:20': 2156, '1:25': 2700, '1:30': 1170, '1:35': 1858, '1:40': 3002, '1:45': 1336, '1:50': 162, '1:55': 2525, '1st': 1013, '2': 17, '2 feet': 1044, '2 hours': 2430, '2 men': 2254, '2 people': 2170, '2 years': 2587, '2.00': 489, '20': 686, '20 feet': 1308, '20 ft': 2089, '200': 723, '2000': 465, '2007': 1909, '2008': 907, '2009': 2232, '2010': 1374, '2011': 2309, '2012': 1090, '2013': 1895, '2015': 1352, '2016': 1378, '21': 1303, '22': 1275, '23': 1189, '24': 99, '25': 513, '26': 2424, '27': 1398, '28': 1549, '29': 951, '2:00': 1187, '2:05': 1407, '2:10': 2371, '2:15': 2246, '2:20': 401, '2:25': 1738, '2:30': 2092, '2:35': 1305, '2:40': 1544, '2:45': 2330, '2:50': 2615, '2:55': 1472, '2nd': 2965, '3': 32, '3 feet': 1096, '3 inches': 1284, '30': 261, '30 mph': 2786, '300': 770, '31': 276, '32': 1401, '33': 982, '34': 913, '35': 663, '350': 2228, '36': 731, '37': 1203, '38': 1932, '39': 2108, '3:00': 1927, '3:10': 2247, '3:15': 2263, '3:20': 1286, '3:25': 564, '3:30': 2747, '3:45': 2417, '3:50': 2765, '3:55': 427, '3rd': 1516, '4': 53, '4 feet': 1141, '4 ft': 3070, '4 inches': 706, '4 way': 2056, '40': 811, '400': 2489, '41': 2478, '42': 1703, '43': 2390, '44': 656, '45': 607, '46': 411, '47': 2605, '48': 1746, '49': 3096, '4:00': 1276, '4:05': 2449, '4:15': 1906, '4:20': 2158, '4:30': 675, '4:35': 599, '4:40': 2982, '4:45': 1805, '4:50': 2956, '4:55': 2230, '4th of july': 1114, '5': 38, '5 feet': 1384, '5 ft': 531, '5 star': 1525, '5 years': 1007, '50': 181, '50 feet': 2157, '500': 1372, '51': 1910, '52': 2013, '53': 1265, '54': 2899, '55': 278, '56': 1773, '59': 3055, '5:00': 2164, '5:05': 2876, '5:10': 2980, '5:15': 2025, '5:18': 2871, '5:25': 1561, '5:30': 2986, '5:40': 2991, '5:45': 1730, '5:50': 2692, '5:55': 997, '6': 90, '6 feet': 2030, '6 inches': 1261, '60': 1254, '600': 2161, '61': 1392, '64': 2817, '65': 2895, '66': 407, '68': 568, '6:00': 2479, '6:05': 3071, '6:20': 560, '6:25': 2113, '6:30': 3119, '6:35': 2932, '6:40': 3068, '6:45': 2055, '7': 122, '7 eleven': 1484, '70': 588, '700': 1756, '72': 2812, '75': 1357, '7:00': 2820, '7:05': 2695, '7:10': 2323, '7:25': 793, '7:35': 2852, '7:45': 2827, '7:55': 3082, '8': 255, '8 feet': 1866, '80': 2338, '870': 2693, '88': 1439, '8:00': 97, '8:05': 2921, '8:35': 2803, '8:50': 2911, '8:55': 2930, '9': 471, '90': 2183, '99': 3091, '9:05': 2934, '9:12': 3107, '9:15': 1728, '9:20': 1291, '9:25': 1223, '9:30': 798, '9:35': 2077, '9:45': 1199, '9:50': 1198, '9:55': 1214, 'None': 125, 'aa': 2054, 'above': 1493, 'above door': 2835, 'above sink': 1555, 'above stove': 1556, 'above toilet': 2387, 'abstract': 128, 'accident': 1589, 'acer': 2350, 'across street': 1529, 'adidas': 940, 'adult': 1009, 'adults': 2456, 'advertisement': 2736, 'africa': 165, 'african': 1655, 'african american': 266, 'after': 2396, 'afternoon': 138, 'against wall': 1605, 'age': 1411, 'ahead': 3047, 'air': 1802, 'air canada': 934, 'air conditioner': 699, 'air force': 482, 'air france': 423, 'airplane': 421, 'airplanes': 1520, 'airport': 957, 'alaska': 1494, 'alcohol': 2329, 'alive': 550, 'all': 1827, 'all of them': 2372, 'all way': 2292, 'alligator': 2877, 'almonds': 2287, 'alps': 1084, 'aluminum': 1955, 'am': 1074, 'amazon': 2844, 'ambulance': 1918, 'america': 1078, 'american': 474, 'american airlines': 3004, 'american flag': 2006, 'amtrak': 2610, 'ana': 1646, 'analog': 1077, 'angel': 2753, 'angels': 925, 'angry': 2415, 'animal': 2171, 'animals': 1752, 'ankle': 2746, 'anniversary': 2913, 'antelope': 966, 'antenna': 2764, 'antique': 1954, 'apartment': 1402, 'apartments': 2885, 'apple': 416, 'apple and banana': 2810, 'apples': 776, 'apron': 197, 'arabic': 2034, 'arch': 2066, 'arizona': 2792, 'arm': 1623, 'army': 2038, 'around neck': 1631, 'arriving': 1677, 'arrow': 2185, 'arrows': 1880, 'art': 1819, 'ascending': 2787, 'asia': 1795, 'asian': 1601, 'asics': 2840, 'asleep': 1212, 'asparagus': 1006, 'asphalt': 390, 'at camera': 1267, 'at table': 2063, 'at&t': 1780, 'athletics': 974, 'atv': 756, 'audi': 1920, 'australia': 889, 'avocado': 1132, 'awake': 1761, 'away': 1501, 'b': 1160, 'babies': 2559, 'baby': 343, \"baby's breath\": 2923, 'back': 353, 'back left': 2643, 'background': 514, 'backhand': 2433, 'backpack': 310, 'backward': 3114, 'backwards': 2495, 'backyard': 273, 'bacon': 1263, 'bad': 661, 'badminton': 2868, 'bag': 87, 'bagel': 2204, 'bagels': 1047, 'baggage claim': 922, 'bags': 1945, 'baked': 2854, 'baker': 2719, 'bakery': 1361, 'baking': 1905, 'balance': 1592, 'balcony': 2135, 'bald': 1394, 'ball': 760, 'balloon': 1706, 'balloons': 1289, 'balls': 1689, 'bamboo': 1383, 'banana': 50, 'banana bread': 2860, 'banana peel': 2469, 'banana split': 2625, 'bananas': 295, 'band': 1734, 'bandana': 1339, 'bank': 2276, 'bank of america': 2472, 'bar': 1579, 'barbed wire': 635, 'barber shop': 1550, 'bark': 2293, 'barn': 363, 'barrel': 753, 'barrier': 2341, 'bars': 2044, 'base': 2295, 'baseball': 66, 'baseball bat': 1691, 'baseball cap': 130, 'baseball field': 1136, 'baseball game': 2357, 'baseball glove': 1453, 'baseball player': 2059, 'baseball uniform': 2180, 'basil': 2128, 'basket': 320, 'basketball': 1249, 'baskets': 2543, 'bat': 371, 'bathing': 1603, 'bathing suit': 1979, 'bathroom': 91, 'bathtub': 2613, 'batman': 2698, 'bats': 573, 'batter': 978, 'batting': 979, 'beach': 263, 'beads': 3115, 'beagle': 1583, 'beanie': 1682, 'beans': 1030, 'bear': 31, 'beard': 341, 'bears': 354, 'bed': 212, 'bedroom': 250, 'beef': 243, 'beer': 68, 'beets': 703, 'before': 2763, 'behind': 2421, 'behind bench': 3009, 'behind bus': 1379, 'behind clouds': 1712, 'behind fence': 2043, 'behind woman': 2756, 'beige': 340, 'beijing': 2645, 'bell': 1444, 'below': 2389, 'belt': 2235, 'bench': 288, 'benches': 1129, 'bending': 1356, 'berries': 1608, 'best buy': 2140, 'bib': 2279, 'bible': 2499, 'bicycle': 379, 'bicycles': 1642, 'bidet': 2890, 'big': 583, 'big ben': 1093, 'bike': 328, 'bike rack': 845, 'biker': 1879, 'bikers': 3077, 'bikes': 1878, 'biking': 1779, 'bikini': 1754, 'billabong': 1467, 'bin': 3031, 'biplane': 397, 'bird': 228, 'bird feeder': 2367, 'birds': 447, 'birthday': 144, 'birthday cake': 1983, 'birthday party': 2670, 'black': 30, 'black and blue': 1753, 'black and brown': 2701, 'black and gray': 2650, 'black and orange': 457, 'black and pink': 2830, 'black and red': 512, 'black and silver': 653, 'black and white': 10, 'black and yellow': 311, 'black white': 2507, 'blackberry': 1659, 'blanket': 1070, 'blankets': 1527, 'bleachers': 846, 'blender': 1131, 'blending': 1462, 'blinders': 1454, 'blinds': 789, 'blonde': 937, 'blood': 1299, 'blt': 2970, 'blue': 13, 'blue and black': 2467, 'blue and gray': 2280, 'blue and green': 1477, 'blue and orange': 2298, 'blue and pink': 2566, 'blue and red': 1629, 'blue and white': 542, 'blue and yellow': 170, 'blue jay': 1231, 'blue team': 2130, 'blueberries': 627, 'blueberry': 2673, 'blurry': 1690, 'bmw': 912, 'bnsf': 1338, 'board': 701, 'boarding': 1243, 'boardwalk': 2743, 'boat': 139, 'boating': 556, 'boats': 1614, 'bob': 1794, 'bone': 2376, 'boogie board': 1915, 'book': 1489, 'books': 1025, 'bookshelf': 2144, 'boot': 1649, 'boots': 455, 'bored': 3097, 'boredom': 2123, 'boston': 1208, 'both': 462, 'bottle': 306, 'bottles': 1578, 'bottom': 69, 'bottom left': 2785, 'bottom right': 1067, 'boundaries': 2894, 'bow': 422, 'bow tie': 2306, 'bowl': 829, 'bowling': 2026, 'bowls': 3046, 'bowtie': 1449, 'box': 605, 'boxer': 1102, 'boxes': 591, 'boxing': 1982, 'boy': 806, 'boys': 472, 'brace': 975, 'bracelet': 688, 'braid': 3088, 'branch': 480, 'branches': 1828, 'brand': 2058, 'brass': 2461, 'braves': 1156, 'brazil': 399, 'bread': 673, 'breakfast': 1216, 'brewers': 1482, 'brick': 240, 'bricks': 1814, 'bride': 3035, 'bridge': 367, 'bridle': 879, 'briefcase': 2784, 'bright': 612, 'britain': 2272, 'british': 893, 'british airways': 885, 'broadway': 2626, 'broccoli': 173, 'broccoli and carrots': 3093, 'broke': 3054, 'broken': 652, 'bronze': 2896, 'broom': 1248, 'brown': 59, 'brown and black': 1247, 'brown and white': 378, 'brush': 2569, 'brushing': 206, 'brushing hair': 2336, 'brushing her teeth': 1526, 'brushing his teeth': 1512, 'brushing teeth': 8, 'bucket': 2283, 'bud light': 1720, 'budweiser': 2642, 'buffalo': 1939, 'building': 153, 'buildings': 2178, 'bull': 1862, 'bulldog': 2527, 'bun': 56, 'bundt': 2538, 'bunk': 2060, 'bunny': 1762, 'bunt': 2408, 'buoy': 1098, 'buoys': 3065, 'burger': 1322, 'burgers': 2597, 'burrito': 2706, 'burton': 895, 'bus': 136, 'bus driver': 3084, 'bus station': 2842, 'bus stop': 376, 'buses': 2579, 'bush': 832, 'bushes': 2305, 'business': 749, 'busy': 1353, 'butt': 1975, 'butter': 109, 'butterfly': 1811, 'button': 1657, 'button up': 1735, 'buttons': 2418, 'by window': 1917, 'c': 1799, 'cabbage': 695, 'cabinet': 1575, 'cabinets': 1003, 'cactus': 780, 'cadillac': 3056, 'cafe': 2076, 'cage': 1005, 'cake': 49, 'cakes': 1792, 'calendar': 2392, 'calico': 1749, 'california': 2302, 'calm': 1287, 'camel': 595, 'camera': 339, 'cameraman': 2380, 'cameras': 2458, 'camo': 2768, 'camouflage': 2277, 'camper': 1416, 'camping': 1498, 'can': 726, \"can't see\": 680, \"can't see it\": 2438, \"can't tell\": 1643, 'canada': 932, 'candle': 561, 'candles': 755, 'candy': 601, 'cane': 508, 'cannot tell': 1923, 'canoe': 1949, 'canon': 2366, 'canopy': 1606, 'cantaloupe': 434, 'cap': 2781, 'captivity': 2655, 'car': 124, 'caramel': 1783, 'cardboard': 816, 'cardinal': 3010, 'cardinals': 2200, 'cargo': 126, 'carnation': 417, 'carnations': 2906, 'carpet': 332, 'carriage': 2504, 'carrot': 319, 'carrot cake': 2823, 'carrots': 440, 'cars': 293, 'cart': 2016, 'cartoon': 2149, 'case': 1211, 'casserole': 2229, 'cast iron': 1957, 'castle': 1948, 'casual': 1147, 'cat': 211, 'cat and dog': 1052, 'cat food': 639, 'catch': 634, 'catch ball': 2318, 'catch frisbee': 1719, 'catcher': 67, 'catching': 977, 'catching frisbee': 1718, 'catholic': 1731, 'cats': 1667, 'caucasian': 1395, 'cauliflower': 1797, 'caution': 505, 'cd': 2335, 'cds': 2938, 'ceiling': 468, 'celery': 617, 'cell': 1660, 'cell phone': 246, 'cell phones': 790, 'cement': 1587, 'center': 1111, 'ceramic': 1235, 'cereal': 1628, 'cessna': 1027, 'chain': 1043, 'chain link': 633, 'chains': 1409, 'chair': 548, 'chairs': 1015, 'chalk': 1984, 'champagne': 2530, 'chandelier': 1028, 'charging': 1393, 'chase': 2811, 'checkerboard': 2684, 'checkered': 813, 'checkers': 855, 'cheddar': 2032, 'cheese': 140, 'cheesecake': 2304, 'chef': 51, 'cherries': 2713, 'cherry': 674, 'chest': 2545, 'chevrolet': 1474, 'chevron': 920, 'chevy': 1911, 'chicago': 1843, 'chicken': 281, 'chihuahua': 1234, 'child': 1314, 'children': 242, 'chili': 2652, 'chimney': 1748, 'china': 791, 'china airlines': 768, 'chinese': 732, 'chips': 2498, 'chiquita': 1186, 'chocolate': 222, 'choppy': 2567, 'chopsticks': 727, 'christian': 2942, 'christmas': 84, 'christmas tree': 896, 'chrome': 1793, 'church': 1264, 'cigarette': 964, 'cigarettes': 1365, 'cilantro': 3041, 'cinnamon': 3030, 'circle': 393, 'circles': 2000, 'circus': 737, 'cirrus': 466, 'citizen': 3092, 'city': 377, 'city bus': 2878, 'clams': 1611, 'classic': 2908, 'classroom': 2589, 'clay': 644, 'clean': 1362, 'cleaner': 2806, 'cleaning': 2314, 'clear': 248, 'cleats': 1596, 'climbing': 2007, 'clip': 2191, 'clock': 80, 'clock tower': 996, 'clocks': 1935, 'close': 2994, 'close up': 2796, 'closed': 1094, 'closet': 2069, 'cloth': 559, 'clothes': 500, 'clothing': 2068, 'cloud': 1835, 'clouds': 57, 'cloudy': 833, 'club': 1446, 'cluttered': 2481, 'clydesdale': 2233, 'cnn': 2800, 'coach': 1708, 'coal': 1435, 'coaster': 2155, 'coat': 1681, 'coats': 2403, 'cobblestone': 1546, 'coca cola': 836, 'cocker spaniel': 1941, 'coconut': 1807, 'coffee': 670, 'coffee cup': 3105, 'coffee maker': 1277, 'coffee pot': 2073, 'coffee table': 2586, 'coins': 2452, 'coke': 1108, 'cold': 215, 'coleslaw': 2061, 'colgate': 1540, 'collage': 2581, 'collar': 256, 'collie': 1875, 'color': 1117, 'colorado': 2023, 'colored': 3033, 'comcast': 3080, 'comfort': 666, 'comforter': 2801, 'coming': 1683, 'commercial': 856, 'commuter': 2809, 'compaq': 2731, 'competition': 3111, 'computer': 29, 'computers': 1002, 'concentration': 2262, 'concert': 2399, 'concrete': 618, 'condiments': 2524, 'conductor': 2464, 'cone': 2019, 'cones': 779, 'conference': 1427, 'conference room': 3064, 'confused': 1228, 'congratulations': 1414, 'construction': 729, 'container': 297, 'continental': 1764, 'control': 1851, 'controller': 1429, 'controllers': 2759, 'converse': 2375, 'cook': 2207, 'cooked': 2053, 'cookie': 820, 'cookies': 2416, 'cooking': 199, 'cool': 2686, 'cooler': 2537, 'copper': 2690, 'copyright': 2429, 'cord': 1744, 'corgi': 1123, 'corn': 865, 'corner': 1500, 'corona': 497, 'cosmo': 3116, 'costume': 694, 'cotton': 714, 'couch': 626, 'counter': 872, 'country': 1621, 'countryside': 2050, 'couple': 2095, 'court': 391, 'cover': 1947, 'cow': 252, 'cowboy': 880, 'cows': 176, 'crafts': 1116, 'crane': 2368, 'cranes': 2945, 'crates': 2774, 'cream': 1951, 'crest': 2838, 'crib': 2624, 'crocs': 2496, 'croissant': 818, 'cross': 183, 'cross country': 1017, 'crossing': 2120, 'crosstown': 3127, 'crosswalk': 133, 'crow': 380, 'crown': 2239, 'crows': 2996, 'cruise ship': 1772, 'csx': 2749, 'cubs': 2519, 'cucumber': 679, 'cucumbers': 2752, 'cuddling': 804, 'cumulus': 217, 'cup': 1219, 'cupcake': 172, 'cupcakes': 2284, 'cups': 938, 'curb': 2109, 'curious': 2505, 'curly': 1625, 'current': 2975, 'curtain': 2117, 'curtains': 385, 'curved': 970, 'cushion': 2985, 'cut': 1553, 'cute': 2420, 'cutting': 342, 'cutting board': 433, 'cutting cake': 1479, 'cutting hair': 579, 'cycling': 2691, 'cylinder': 2856, 'd': 2491, 'dachshund': 2665, 'dad': 1459, 'daffodil': 3067, 'daffodils': 1890, 'dairy': 2260, 'dairy queen': 2572, 'daisies': 1055, 'daisy': 382, 'dalmatian': 1558, 'dancing': 2604, 'dandelions': 1863, 'dark': 908, 'dawn': 2638, 'day': 905, 'day time': 705, 'daytime': 990, 'db': 2922, 'dc': 2301, 'dead': 2065, 'dead end': 1977, 'deck': 2154, 'decoration': 750, 'decorative': 1419, 'deep': 1898, 'deer': 2521, 'defense': 2639, 'deli': 2598, 'delivery': 2808, 'dell': 454, 'delta': 657, 'denim': 915, 'descending': 2907, 'desert': 1884, 'design': 1774, 'desk': 461, 'desktop': 1205, 'dessert': 1355, 'desserts': 1206, 'detroit': 1816, 'diamond': 1130, 'diamonds': 1140, 'diesel': 2847, 'diet coke': 1162, 'different teams': 2199, 'digital': 3074, 'dim': 2735, 'dining': 2635, 'dining room': 1399, 'dinner': 528, 'dinosaur': 388, 'dip': 3081, 'direction': 2440, 'directions': 2052, 'dirt': 541, 'dirt bike': 2771, 'dirty': 486, 'dishes': 1557, 'dishwasher': 1892, 'disney': 2958, 'display': 1244, 'distance': 2951, 'do not enter': 2732, 'dock': 58, 'dodge': 2405, 'dodgers': 1519, 'dog': 22, 'dog and cat': 1049, 'dog bed': 2992, 'dog food': 2296, 'dog show': 2482, 'dogs': 1061, 'dole': 2422, 'doll': 1889, 'dome': 1185, 'domestic': 887, \"don't know\": 1341, \"don't walk\": 2588, 'donkey': 2937, 'donut': 783, 'donut shop': 1747, 'donuts': 355, 'door': 744, 'doorway': 2659, 'dots': 227, 'double': 2012, 'double decker': 358, 'doubles': 1900, 'dough': 3019, 'doughnut': 1037, 'doughnuts': 2471, 'down': 267, 'down street': 2308, 'downhill': 567, 'downtown': 1600, 'dr pepper': 2369, 'dragon': 2126, 'drain': 3049, 'drawer': 1292, 'drawing': 2248, 'dreadlocks': 2865, 'dress': 534, 'dresser': 1415, 'drink': 843, 'drinking': 838, 'drinking water': 2705, 'drinks': 2534, 'drive': 1721, 'driver': 921, 'driveway': 2804, 'driving': 494, 'drums': 2485, 'dry': 1107, 'drying': 3061, 'drywall': 1317, 'ducati': 2288, 'duck': 1531, 'ducks': 1035, 'dugout': 960, 'dump': 620, 'dump truck': 1018, 'dunkin donuts': 1029, 'dusk': 284, 'e': 98, 'each other': 2881, 'eagle': 797, 'ear': 1908, 'earbuds': 2106, 'earring': 1242, 'earrings': 1658, 'ears': 1310, 'east': 1480, 'easter': 1323, 'easton': 2901, 'easy': 2488, 'easyjet': 2406, 'eat': 442, 'eaten': 2018, 'eating': 36, 'egg': 2070, 'egg salad': 2758, 'eggs': 348, 'eiffel tower': 1695, 'electric': 402, 'electricity': 526, 'electronics': 2024, 'elephant': 74, 'elephants': 290, 'elm': 64, 'elmo': 2966, 'email': 1996, 'emergency': 2475, 'emirates': 1785, 'empty': 1011, 'enclosure': 3034, 'end': 2872, 'engine': 594, 'england': 359, 'english': 245, 'entering': 2846, 'equestrian': 1864, 'europe': 2064, 'evening': 283, 'evergreen': 149, 'exhaust': 2201, 'exit': 2748, 'eyes': 308, 'f': 924, 'fabric': 624, 'face': 1369, 'facebook': 1069, 'factory': 914, 'fair': 2805, 'fake': 1812, 'fall': 672, 'falling': 2282, 'family': 1767, 'fan': 903, 'fancy': 2327, 'fans': 2609, 'fanta': 2502, 'far': 849, 'far right': 1776, 'farm': 1016, 'farmer': 1874, 'farmers': 2629, 'farmers market': 2465, 'fashion': 1964, 'fast': 1278, 'fast food': 1729, 'father': 2912, 'faucet': 863, 'feathers': 1496, 'fedex': 2152, 'fedora': 1965, 'feeder': 2386, 'feeding': 2017, 'feeding giraffe': 1320, 'feet': 2446, 'fell': 1209, 'female': 205, 'fence': 233, 'fern': 1666, 'ferris wheel': 710, 'ferry': 1166, 'festival': 734, 'feta': 1832, 'few': 2552, 'field': 445, 'fighter': 2769, 'fighting': 1722, 'finch': 1358, 'finger': 3110, 'fire': 1024, 'fire extinguisher': 2555, 'fire hydrant': 638, 'fire truck': 1504, 'firefighter': 2137, 'fireman': 2554, 'fireplace': 1711, 'fires': 2935, 'first': 1517, 'first base': 1893, 'fish': 179, 'fisheye': 944, 'fishing': 366, 'fishing boat': 2062, 'flag': 676, 'flags': 1886, 'flamingo': 1765, 'flashlight': 2712, 'flat': 752, 'flat screen': 2409, 'flats': 1232, 'flickr': 578, 'flip': 2946, 'flip flops': 1113, 'flip phone': 2788, 'floating': 1952, 'flood': 2841, 'floor': 840, 'floral': 119, 'florida': 1423, 'flour': 1962, 'flower': 769, 'flowers': 118, 'fluffy': 2401, 'fluorescent': 469, 'fly': 1315, 'fly kite': 1842, 'flying': 492, 'flying kite': 651, 'flying kites': 909, 'foam': 2793, 'focus': 1787, 'fog': 1938, 'foggy': 1742, 'foil': 1577, 'food': 334, 'food processor': 2990, 'food truck': 1368, 'foot': 2300, 'football': 537, 'footprints': 1257, 'for balance': 498, 'for fun': 917, 'for photo': 2550, 'for sale': 1622, 'ford': 546, 'foreground': 766, 'forehand': 2348, 'forest': 945, 'fork': 251, 'fork and knife': 224, 'fork and spoon': 2115, 'forks': 720, 'formal': 2188, 'formica': 3044, 'forward': 312, 'fountain': 1071, 'fox': 2394, 'frame': 1845, 'france': 487, 'free': 757, 'freezer': 2257, 'freight': 1218, 'freightliner': 3128, 'french': 1568, 'french fries': 2100, 'fresh': 1229, 'fridge': 609, 'fried': 2961, 'friend': 415, 'friends': 1582, 'fries': 711, 'frisbee': 7, 'frisbees': 1072, 'frog': 1913, 'front': 1056, 'frosted': 1095, 'frosting': 2266, 'fruit': 432, 'fruit salad': 2918, 'fruits': 1290, 'full': 610, 'fun': 1311, 'fur': 1279, 'furniture': 2354, 'futon': 3086, 'g': 2244, 'game': 980, 'game controller': 2454, 'gaming': 1146, 'garage': 2322, 'garbage': 961, 'garden': 2286, 'garlic': 1514, 'gas': 735, 'gas station': 1881, 'gate': 2192, 'gatorade': 1201, 'gazebo': 2618, 'ge': 2574, 'geese': 2508, 'genetics': 2662, 'german': 1282, 'german shepherd': 1040, 'germany': 1283, 'ghost': 2210, 'giants': 1781, 'ginger': 2983, 'giraffe': 168, 'giraffe and zebra': 2826, 'giraffes': 441, 'girl': 451, 'girl on right': 2703, 'girls': 1152, 'give way': 1848, 'glass': 709, 'glasses': 34, 'glaze': 1333, 'glazed': 1560, 'globe': 2721, 'glove': 420, 'gloves': 459, 'gmc': 801, 'go': 875, 'goal': 3021, 'goalie': 1142, 'goat': 1669, 'goatee': 716, 'goats': 1412, 'goggles': 824, 'going': 1076, 'gold': 902, 'golden gate': 1946, 'golden retriever': 1668, 'golf': 2022, 'gone': 2669, 'good': 161, 'google': 2742, 'goose': 1865, 'gothic': 1304, 'graduation': 1796, 'graffiti': 967, 'grandfather': 2223, 'granite': 1684, 'grape': 2540, 'grapefruit': 1215, 'grapes': 923, 'grass': 45, 'grassy': 1059, 'gravel': 1057, 'gravy': 2114, 'gray': 16, 'gray and black': 2402, 'gray and red': 1178, 'gray and white': 1490, 'grazing': 101, 'green': 14, 'green and black': 765, 'green and blue': 1569, 'green and brown': 2999, 'green and orange': 864, 'green and red': 2340, 'green and white': 1148, 'green and yellow': 315, 'green beans': 2175, 'greyhound': 658, 'grill': 265, 'grilled': 1567, 'grilled cheese': 2857, 'grind': 1552, 'grinding': 2570, 'grizzly': 2484, 'grocery': 1736, 'grocery store': 3094, 'ground': 309, 'guitar': 1327, 'guitar hero': 2561, 'gun': 2791, 'gym': 1109, 'h': 1389, 'hair': 758, 'hair dryer': 189, 'haircut': 2745, 'half': 1417, 'half full': 2503, 'halloween': 572, 'hallway': 1725, 'ham': 1262, 'ham and cheese': 2928, 'hamburger': 2173, 'hammer time': 2622, 'hammock': 2739, 'hand': 563, 'handicap': 1840, 'handle': 628, 'handlebars': 927, 'hands': 1627, 'hanger': 1008, 'hanging': 2041, 'happiness': 963, 'happy': 430, 'happy birthday': 719, 'harbor': 2593, 'hard': 2477, 'hardwood': 1385, 'harley': 2147, 'harley davidson': 1434, 'harness': 1662, 'harry potter': 2500, 'hat': 100, 'hats': 1260, 'hauling': 2202, 'hawaii': 2501, 'hawaiian': 1420, 'hawk': 2031, 'hay': 154, 'hazy': 2364, \"he isn't\": 1478, \"he's not\": 2608, 'head': 1316, 'headband': 1638, 'headphones': 1217, 'healthy': 2974, 'heart': 437, 'hearts': 1672, 'heat': 2474, 'heater': 1602, 'heavy': 2238, 'heels': 1296, 'heineken': 2798, 'heinz': 2950, 'helicopter': 2132, 'hello kitty': 900, 'helmet': 286, 'helmets': 949, 'herd': 2011, 'herding': 177, 'herself': 1173, 'hexagon': 2241, 'hiding': 3020, 'high': 2014, 'high chair': 1852, 'high heels': 2682, 'highway': 523, 'hiking': 2343, 'hill': 2358, 'hills': 2091, 'hilly': 3106, 'himself': 1999, 'hispanic': 3121, 'hit': 2125, 'hit ball': 1106, 'hitting': 2972, 'hitting ball': 2492, 'hockey': 2726, 'holding': 2824, 'holding baby': 1251, 'holding it': 557, 'holding phone': 2630, 'holding umbrella': 2697, 'hollywood': 3112, 'home': 448, 'home plate': 2294, 'homemade': 603, 'honda': 488, 'honey': 1739, 'hood': 1155, 'hoodie': 2445, 'horizontal': 446, 'horizontally': 2775, 'horns': 1543, 'horse': 264, 'horse racing': 2831, 'horseback riding': 1760, 'horses': 848, 'hose': 562, 'hospital': 1163, 'hot': 2419, 'hot dog': 106, 'hot dogs': 194, 'hot sauce': 305, 'hotel': 1240, 'hotel room': 3099, 'house': 544, 'houses': 587, 'hp': 460, 'hsbc': 2914, 'htc': 2973, 'huge': 2205, 'hugging': 1925, 'human': 1238, 'humans': 669, 'hummingbird': 993, 'hundreds': 2352, 'hungry': 1916, 'husky': 1678, 'hydrant': 1823, 'i': 2971, \"i don't know\": 1408, 'ibm': 2710, 'ice': 158, 'ice cream': 1473, 'icing': 2258, 'identification': 1463, 'illinois': 1620, 'in': 742, 'in air': 269, 'in back': 3038, 'in background': 2080, 'in basket': 2780, 'in bowl': 831, 'in box': 1613, 'in cabbage town': 3122, 'in car': 1062, 'in corner': 2240, 'in cup': 3017, 'in field': 1902, 'in front': 1367, 'in grass': 2189, 'in hand': 2487, 'in her hand': 2963, 'in his hand': 2480, 'in middle': 1041, 'in motion': 2423, 'in sink': 2274, 'in sky': 869, 'in snow': 1798, 'in stands': 1969, 'in street': 962, 'in suitcase': 1593, 'in vase': 1321, 'in water': 852, 'index': 1869, 'india': 1637, 'indian': 871, 'indians': 2933, 'indoor': 1953, 'indoors': 2072, 'information': 2363, 'inside': 146, 'intersection': 2494, 'iphone': 414, 'ipod': 1363, 'ireland': 2289, 'iris': 2243, 'iron': 2585, 'island': 1298, \"it isn't\": 2176, \"it's not\": 1850, \"it's raining\": 1686, 'italian': 1118, 'italy': 527, 'ivory': 520, 'ivy': 1791, 'j': 1562, 'jacket': 1487, 'jackets': 313, 'jal': 3118, 'japan': 861, 'japanese': 985, 'jar': 2897, 'jeans': 725, 'jeep': 2377, 'jelly': 2571, 'jesus': 2583, 'jet': 1350, 'jet ski': 2708, 'jetblue': 2267, 'jets': 2531, 'jockey': 1486, 'john': 2723, 'jones': 1704, 'joshua': 2595, 'jp morgan': 2165, 'juice': 307, 'jump': 1506, 'jumped': 386, 'jumping': 646, 'jungle': 1607, 'junk': 2520, 'k': 1497, 'kangaroo': 2637, 'kawasaki': 2096, 'kayak': 2265, 'kayaking': 2654, 'kenmore': 2919, 'ketchup': 784, 'ketchup and mustard': 2028, 'kettle': 662, 'keyboard': 991, 'keys': 2864, 'khaki': 1673, 'kia': 2002, 'kicking': 2599, 'kickstand': 1253, 'kid': 1390, 'kids': 687, 'king': 1580, 'kissing': 2432, 'kitchen': 198, 'kitchenaid': 2866, 'kite': 649, 'kite flying': 828, 'kite string': 2967, 'kites': 216, 'kitesurfing': 1717, 'kiting': 1230, 'kitten': 1888, 'kiwi': 435, 'klm': 543, 'knee pads': 2532, 'kneeling': 2614, 'knife': 48, 'knife and fork': 209, 'knives': 1190, 'kodak': 3005, 'korean air': 3102, 'krispy kreme': 2379, 'l': 2511, 'la': 540, 'lab': 444, 'labrador': 1808, 'lace': 2600, 'lacoste': 901, 'ladder': 708, 'lady': 2404, 'ladybug': 2088, 'lake': 1450, 'lamb': 2005, 'lamp': 678, 'lamps': 1222, 'land': 1857, 'landing': 1930, 'landscape': 1445, 'lanes': 1877, 'lanyard': 2214, 'lap': 654, 'laptop': 27, 'laptops': 1585, 'large': 25, 'laughing': 805, 'laundry': 159, 'laying': 625, 'laying down': 1926, 'lays': 2621, 'leaf': 943, 'leaning': 2728, 'learning': 2987, 'leash': 1688, 'leather': 1066, 'leaves': 88, 'leaving': 2836, 'left': 40, 'left 1': 597, 'left and right': 1656, 'left side': 3058, 'leg': 1300, 'lego': 2751, 'legos': 2704, 'legs': 1410, 'lemon': 1381, 'lemonade': 1617, 'lemons': 2557, 'leopard': 2535, 'letters': 1891, 'lettuce': 333, 'lexus': 1726, 'lg': 2398, 'library': 2564, 'license plate': 2995, 'licking': 642, 'lid': 2607, 'life': 1063, 'life jacket': 2855, 'life vest': 1099, 'lifeguard': 1859, 'lift': 2414, 'light': 137, 'lighter': 1354, 'lighthouse': 1038, 'lighting': 697, 'lights': 707, 'lilac': 2194, 'lilies': 2075, 'lily': 2679, 'lime': 2118, 'limes': 3079, 'lines': 1347, 'linoleum': 1551, 'lion': 2767, 'liquor': 862, 'listening': 1624, 'listening to music': 2339, 'little': 939, 'little girl': 2250, 'living': 2782, 'living room': 41, 'lizard': 1616, 'loading': 1509, 'lobster': 771, 'log': 37, 'logitech': 2370, 'logo': 400, 'logs': 1882, 'london': 574, 'long': 954, 'long sleeve': 2869, 'long time': 1233, 'looking': 2145, 'looking at camera': 2160, 'looking at phone': 1366, 'looking out window': 2281, 'los angeles': 368, 'lot': 641, 'lotion': 2699, 'lots': 1768, 'love': 2426, 'low': 822, 'lufthansa': 2345, 'luggage': 956, 'lunch': 108, 'lying down': 2680, 'm': 1618, 'mac': 2224, 'macaroni': 2952, 'machine': 2051, 'mack': 730, 'magazine': 1716, 'magazines': 381, 'magnet': 2225, 'magnets': 1789, 'mailbox': 2510, 'main': 1207, 'main st': 1226, 'main street': 2317, 'makeup': 984, 'male': 219, 'males': 1442, 'mall': 2616, 'man': 231, 'man in middle': 3083, 'man made': 2677, 'man on left': 1396, 'man on right': 1674, \"man's\": 1817, 'mane': 1087, 'mango': 1693, 'mantle': 1023, 'many': 204, 'map': 1241, 'maple': 722, 'maple leaf': 2714, 'marble': 1844, 'marina': 2497, 'mariners': 3089, 'mario': 2319, 'marker': 2459, 'market': 1337, 'maroon': 2989, 'married': 2783, 'marshmallows': 1782, 'mask': 690, 'mat': 1790, 'mattress': 2003, 'mayo': 114, 'mayonnaise': 113, \"mcdonald's\": 1675, 'me': 1440, 'meat': 788, 'meatballs': 2858, 'medium': 1101, 'meeting': 2819, 'men': 1431, \"men's\": 1545, 'menu': 2347, 'meow': 536, 'mercedes': 868, 'mercedes benz': 1174, 'messy': 1563, 'metal': 232, 'meter': 1391, 'metro': 2435, 'mets': 326, 'mexican': 1976, 'mexico': 1653, 'miami': 2839, 'michigan': 2993, 'mickey mouse': 691, 'microphone': 352, 'microsoft': 1921, 'microwave': 800, 'middle': 796, 'middle 1': 2898, 'military': 1609, 'milk': 1165, 'millions': 2307, 'minnie mouse': 602, 'mint': 2395, 'mirror': 786, 'mirrors': 1872, 'mississippi': 2828, 'mitsubishi': 2939, 'mitt': 2453, 'mixer': 1460, 'model': 2738, 'modern': 452, 'mohawk': 2442, 'mom': 1599, 'monday': 2539, 'money': 2332, 'monitor': 1443, 'monkey': 1022, 'monster': 2861, 'moon': 1426, 'moped': 733, 'more': 2955, 'morning': 73, 'mosaic': 2762, 'moss': 2045, 'motel': 302, 'mother': 1654, 'mother and child': 2325, 'motion': 404, 'motocross': 1714, 'motor': 1971, 'motorbike': 1876, 'motorcycle': 15, 'motorcycles': 664, 'motorola': 1158, 'mound': 2242, 'mountain': 549, 'mountain dew': 1188, 'mountainous': 2715, 'mountains': 337, 'mouse': 604, 'mouse pad': 1834, 'mouth': 1343, 'mouthwash': 2949, 'movement': 2737, 'movie': 606, 'moving': 1227, 'mozzarella': 636, 'mt airy': 3126, 'mud': 2133, 'muffin': 1732, 'muffins': 2448, 'mug': 1694, 'multi': 1091, 'multi colored': 2565, 'multicolored': 873, 'multiple': 3029, 'mural': 3006, 'museum': 683, 'mushroom': 581, 'mushrooms': 1236, 'music': 2411, 'mustache': 1436, 'mustard': 55, 'mutt': 1048, 'n': 2687, 'name': 814, 'name tag': 812, 'napkin': 897, 'napkins': 1246, 'nasa': 2365, \"nathan's\": 1743, 'national express': 2162, 'natural': 1528, 'nature': 854, 'navy': 2151, 'neck': 2124, 'necklace': 89, 'neither': 203, 'neon': 1833, 'nest': 3028, 'net': 0, 'never': 346, 'new': 1936, 'new orleans': 3032, 'new york': 1468, 'news': 2186, 'newspaper': 1534, 'next to toilet': 807, 'night': 677, 'night time': 403, 'nightstand': 180, 'nighttime': 886, 'nike': 71, 'nikon': 1595, 'nintendo': 2439, 'nissan': 1987, 'no': 9, 'no 1': 413, 'no cat': 2529, 'no clock': 2218, 'no dog': 2046, 'no flag': 1032, 'no grass': 1574, 'no hat': 1128, 'no left turn': 2926, 'no light': 3011, 'no man': 745, 'no number': 877, 'no parking': 241, 'no plate': 3022, 'no shirt': 3051, 'no sign': 1943, 'no smoking': 2264, 'no train': 478, 'no water': 2920, 'nobody': 1221, 'nokia': 2252, 'noodles': 952, 'noon': 504, 'normal': 2998, 'north': 70, 'north america': 2184, 'north face': 1993, 'nose': 2196, 'not': 2486, 'not at all': 650, 'not here': 1970, 'not high': 2863, 'not in service': 2663, 'not likely': 1855, 'not long': 2910, 'not possible': 739, 'not sure': 1995, 'not there': 1139, 'not very': 1639, 'notebook': 2528, 'notes': 2675, 'nothing': 134, 'now': 876, 'nowhere': 188, 'numbers': 1931, 'nursing': 2378, 'nuts': 1894, 'ny': 1775, 'o': 1899, 'oak': 930, 'oar': 2085, 'oars': 1741, 'obama': 2004, 'ocean': 214, 'octagon': 947, 'octopus': 2623, 'off': 524, 'office': 24, 'oil': 724, 'old': 384, 'older': 3027, 'olives': 883, 'ollie': 994, 'olympics': 911, 'omelet': 2237, 'on': 23, 'on beach': 1192, 'on bed': 1168, 'on bench': 772, 'on bike': 1466, 'on boat': 2483, 'on building': 426, 'on bus': 2905, 'on car': 2547, 'on chair': 1089, 'on couch': 600, 'on counter': 299, 'on desk': 1088, 'on dresser': 3036, 'on elephant': 1541, 'on floor': 906, 'on fridge': 1225, 'on grass': 1197, 'on ground': 821, 'on his face': 2473, 'on his head': 717, 'on horse': 1650, 'on laptop': 2667, 'on left': 1809, 'on man': 1922, 'on motorcycle': 1788, 'on napkin': 1813, 'on phone': 2631, 'on pizza': 1860, 'on plane': 2953, 'on plate': 1237, 'on pole': 2315, 'on rack': 1989, 'on right': 1960, 'on road': 969, 'on rock': 2400, 'on runway': 933, 'on shelf': 2179, 'on shore': 2873, 'on sidewalk': 565, 'on sign': 2984, 'on sink': 1597, 'on skateboard': 1679, 'on stove': 2959, 'on street': 464, 'on suitcase': 1334, 'on table': 178, 'on toilet': 844, 'on top': 935, 'on tower': 965, 'on track': 525, 'on tracks': 483, 'on train': 2107, 'on tray': 2203, 'on tree': 1934, 'on wall': 728, 'on water': 2590, 'on woman': 2256, 'onion': 1324, 'onion rings': 1981, 'onions': 111, 'only': 2249, 'opaque': 2656, 'open': 369, 'opponent': 3123, 'orange': 2, 'orange and black': 1110, 'orange and blue': 2182, 'orange and white': 406, 'orange and yellow': 2084, 'orange juice': 1121, 'oranges': 304, 'orchid': 2832, 'oregon': 1386, 'organic': 1388, 'oriental': 2875, 'orioles': 1942, 'ostrich': 2612, 'ottoman': 2522, 'out': 1405, 'out of focus': 2206, 'outdoor': 54, 'outdoors': 1360, 'outfield': 2941, 'outside': 655, 'oval': 682, 'oven': 1293, 'over': 1213, 'over easy': 3040, 'overalls': 2601, 'overcast': 182, 'owl': 1464, 'owner': 1769, 'p': 323, 'pacific': 2681, 'pacifier': 2197, 'packing': 2636, 'paddle': 1978, 'paddle boarding': 1081, 'paddling': 2859, 'paint': 696, 'painted': 2082, 'painting': 1701, 'paisley': 2355, 'pajamas': 2227, 'palm': 477, 'palm tree': 1771, 'palm trees': 2166, 'pan': 837, 'pancake': 2337, 'pancakes': 2797, 'panda': 2658, 'pans': 2425, 'pants': 1535, 'paper': 141, 'paper towels': 1581, 'papers': 1250, 'parachute': 511, 'parade': 553, 'parakeet': 2269, 'parasailing': 275, 'pare': 2683, 'paris': 1533, 'park': 272, 'parked': 532, 'parking': 324, 'parking garage': 2671, 'parking lot': 175, 'parking meter': 1033, 'parking meters': 2944, 'parmesan': 631, 'parmesan cheese': 2255, 'parrot': 229, 'parrots': 481, 'parsley': 1818, 'partly cloudy': 2685, 'party': 195, 'passenger': 475, 'passengers': 424, 'pasta': 1154, 'pastries': 1359, 'pastry': 2195, 'pasture': 1499, 'patio': 2275, 'patterned': 156, 'paved': 3013, 'pavement': 1137, 'paw': 2799, 'pc': 1588, 'peace': 1068, 'peach': 1183, 'peaches': 2086, 'peacock': 2122, 'peanut butter': 1125, 'peanuts': 2558, 'pear': 2428, 'pearl': 2407, 'peas': 1958, 'pedestal': 1045, 'pedestrian': 1929, 'pedestrian crossing': 2099, 'pedestrians': 1438, 'pee': 2381, 'peeing': 2549, 'pelican': 2231, 'pelicans': 2661, 'pen': 1491, 'pencil': 2988, 'penguin': 2515, 'penne': 287, 'pens': 2168, 'people': 192, 'pepper': 586, 'pepperoni': 640, 'peppers': 110, 'pepsi': 160, 'persian': 1896, 'person': 986, 'petting': 809, 'petting horse': 1421, 'philadelphia': 2577, 'phillies': 1870, 'phone': 244, 'phones': 774, 'photo': 2209, 'photograph': 592, 'photographer': 1755, 'photography': 2493, 'photoshop': 2226, 'piano': 1342, 'pickle': 1610, 'pickles': 571, 'pickup': 1151, 'picnic': 1937, 'picnic table': 2101, 'picture': 552, 'pictures': 226, 'pie': 1126, 'pier': 1319, 'pig': 116, 'pigeon': 1825, 'pigeons': 1060, 'pigtails': 1919, 'pillow': 1537, 'pillows': 1663, 'pilot': 1269, 'pine': 235, 'pineapple': 1266, 'ping pong': 799, 'pink': 42, 'pink and black': 1309, 'pink and blue': 2582, 'pink and white': 2215, 'pink and yellow': 247, 'pipe': 629, 'pipes': 2936, 'pirate': 2320, 'pirates': 987, 'pitbull': 1079, 'pitch': 2837, 'pitcher': 1, 'pitching': 959, 'pizza': 123, 'pizza box': 3018, 'pizza cutter': 884, 'pizza hut': 2356, 'placemat': 1822, 'plaid': 988, 'plain': 819, 'plane': 372, 'planes': 516, 'plant': 1829, 'planter': 2651, 'plants': 1740, 'plaster': 773, 'plastic': 392, 'plastic wrap': 2290, 'plate': 107, 'plates': 740, 'platform': 1177, 'play': 613, 'play tennis': 1626, 'player': 1676, 'players': 2027, 'playing': 1014, 'playing baseball': 316, 'playing frisbee': 1119, 'playing game': 1710, 'playing soccer': 473, 'playing tennis': 259, 'playing video game': 2702, 'playing video games': 2772, 'playing wii': 1193, 'playstation': 1853, 'plow': 2707, 'plunger': 2776, 'pm': 2848, 'pocket': 2517, 'pockets': 3066, 'pointing': 1873, 'polar': 874, 'polar bear': 1272, 'polar bears': 2412, 'pole': 132, 'poles': 20, 'police': 785, 'police officer': 2310, 'polka dot': 262, 'polka dots': 2383, 'polo': 1784, 'pomeranian': 2724, 'pond': 615, 'pony': 1135, 'ponytail': 748, 'poodle': 330, 'pool': 1598, 'poop': 2513, 'pooping': 2116, 'poor': 1051, 'porcelain': 681, 'porch': 2851, 'pork': 2509, 'posing': 623, 'post': 2526, 'poster': 566, 'posts': 2592, 'pot': 1461, 'potato': 2757, 'potato salad': 2591, 'potatoes': 647, 'pots': 1288, 'pottery': 2627, 'powdered': 2640, 'powdered sugar': 2098, 'power': 858, 'power lines': 1239, 'practice': 2845, 'prince': 3057, 'print': 718, 'printer': 1054, 'privacy': 2278, 'private': 853, 'produce': 665, 'professional': 493, 'prom': 2359, 'propeller': 76, 'protection': 1432, 'protest': 496, 'public': 929, 'public market center': 2142, 'pug': 891, 'pull': 826, 'puma': 2943, 'pumpkin': 1837, 'puppy': 1968, 'purple': 163, 'purple and white': 2879, 'purse': 18, 'qantas': 3100, 'qatar': 2212, 'queen': 1757, 'quilt': 1026, 'r': 2049, 'rabbit': 1010, 'race': 1050, 'racing': 389, 'rack': 2657, 'racket': 577, 'rackets': 253, 'racquet': 2755, 'radiator': 2778, 'radio': 842, 'radish': 2733, 'raft': 39, 'rail': 2444, 'railing': 44, 'railroad crossing': 2384, 'rain': 684, 'rainbow': 1167, 'raining': 507, 'rainy': 439, 'ram': 2603, 'ramp': 443, 'ranch': 2382, 'raspberries': 2903, 'raspberry': 948, 'raw': 1042, 'rays': 2153, 'reading': 547, 'real': 142, 'rear': 1058, 'recently': 778, 'recessed': 1867, 'recliner': 2760, 'rectangle': 193, 'rectangles': 2131, 'red': 6, 'red and black': 643, 'red and blue': 394, 'red and gray': 2136, 'red and green': 598, 'red and silver': 1632, 'red and white': 75, 'red and yellow': 187, 'red bull': 1268, 'red light': 971, 'red sox': 370, 'red velvet': 3098, 'red white and blue': 230, 'red white blue': 1492, 'reds': 1424, 'referee': 95, 'reflection': 569, 'refrigerator': 61, 'refrigerators': 2777, 'regular': 1547, 'reins': 1647, 'relaxing': 1080, 'relish': 1031, 'remodeling': 2850, 'remote': 614, 'remote control': 1326, 'remotes': 1153, 'residential': 2594, 'restaurant': 210, 'resting': 289, 'ribbon': 2103, 'rice': 781, 'ride': 1085, 'riding': 317, 'riding bike': 329, 'riding bikes': 1483, 'riding elephant': 1826, 'riding horse': 2413, 'riding horses': 1377, 'riding motorcycle': 1652, 'right': 43, 'right 1': 2744, 'right hand': 2997, 'right side': 2981, 'ring': 200, 'ring finger': 659, 'ripe': 3024, 'river': 277, 'road': 777, 'roast beef': 2009, 'robe': 1630, 'robin': 1430, 'robot': 1698, 'rock': 787, 'rocks': 899, 'rocky': 1847, 'rodeo': 1648, 'rolex': 79, 'roll': 1433, 'roman': 1856, 'roman numerals': 1513, 'roof': 926, 'room': 2455, 'rooster': 1702, 'rope': 1112, 'rose': 989, 'roses': 336, 'rottweiler': 2884, 'rough': 1800, 'round': 223, 'roundabout': 3045, 'rowing': 1838, 'rubber': 1280, 'rug': 129, 'rugby': 538, 'run': 1344, 'running': 361, 'runway': 1144, 'rural': 910, 'russia': 2316, 'russian': 1522, 'rust': 792, 'rv': 2833, 'rye': 1766, 's': 1413, 'sad': 1252, 'saddle': 2104, 'safari': 1371, 'safe': 2813, 'safety': 622, 'sail': 1974, 'sailboat': 775, 'sailboats': 1505, 'sailing': 2568, 'salad': 300, 'salmon': 2039, 'salon': 1786, 'salt': 2523, 'salt and pepper': 882, 'samsung': 1001, 'san diego': 2001, 'san francisco': 998, 'sand': 213, 'sandals': 1036, 'sandwich': 350, 'sandwiches': 1400, 'santa': 751, 'santa hat': 2544, 'sas': 3113, 'sauce': 1849, 'sauerkraut': 1046, 'sausage': 1532, 'savannah': 616, 'savory': 2931, 'scale': 2674, 'scania': 621, 'scarf': 1224, 'scenery': 1985, 'schnauzer': 1387, 'school': 509, 'school bus': 1584, 'scissors': 338, 'scooter': 1075, 'scrambled': 2067, 'scratching': 2874, 'screen': 1428, 'seafood': 1612, 'seagull': 321, 'seagulls': 1448, 'seat': 1998, 'seattle': 2143, 'seaweed': 2829, 'second': 412, 'security': 1172, 'sedan': 2211, 'seeds': 1841, 'selfie': 1950, 'selling': 3087, 'semi': 619, 'sepia': 1351, 'serious': 2794, 'serve': 1641, 'serving': 747, 'sesame': 2216, 'sesame seeds': 2090, 'setting': 1481, 'several': 2261, 'sewing': 2546, 'shade': 558, 'shadow': 279, 'shadows': 147, 'shaking hands': 2573, 'shallow': 398, 'shampoo': 1083, 'shape': 1375, 'shark': 2111, 'shaved': 2822, 'shearing': 1458, 'shed': 2606, 'sheep': 102, 'sheepdog': 2870, 'sheet': 2741, 'sheets': 2802, 'shelf': 1737, 'shell': 1576, 'shells': 1966, 'shelter': 2924, 'shelves': 1376, 'shepherd': 713, 'shih tzu': 2960, 'shingles': 151, 'ship': 1331, 'shirt': 362, 'shirt and tie': 1475, 'shirts': 2385, 'shoe': 2270, 'shoes': 331, 'shop': 1980, 'shopping': 2694, 'shopping cart': 1685, 'shore': 2889, 'short': 839, 'shorter': 1318, 'shorts': 237, 'shoulder': 1255, 'show': 1699, 'shower': 92, 'shower curtain': 2037, 'shower head': 3023, 'shrimp': 449, 'shut': 827, 'siamese': 1115, 'siblings': 2653, 'side': 1086, 'side of road': 1518, 'sidecar': 2512, 'sidewalk': 596, 'sideways': 1751, 'sign': 463, 'signs': 535, 'silk': 3063, 'silver': 60, 'silver and black': 1956, 'silver and red': 2770, 'silverware': 1418, 'singapore': 2553, 'singing': 2904, 'single': 958, 'single engine': 2957, 'singles': 2584, 'sink': 529, 'sitting': 318, 'size': 1861, 'skate': 2900, 'skate park': 1120, 'skateboard': 11, 'skateboarder': 835, 'skateboarding': 431, 'skateboards': 904, 'skatepark': 2917, 'skating': 968, 'skeleton': 2271, 'ski': 1502, 'ski boots': 2578, 'ski lift': 973, 'ski pole': 155, 'ski poles': 322, 'ski resort': 285, 'ski slope': 2476, 'skier': 1157, 'skiers': 2821, 'skiing': 5, 'skirt': 260, 'skis': 19, 'skull': 2119, 'skull and crossbones': 2008, 'sky': 357, 'skyscraper': 1524, 'skyscrapers': 3014, 'slacks': 2818, 'sled': 2462, 'sleep': 1256, 'sleeping': 931, 'sleeve': 2727, 'sliced': 436, 'slide': 2562, 'sliding': 983, 'slippers': 554, 'slope': 2649, 'slow': 815, 'slow down': 2620, 'small': 484, 'smaller': 1883, 'smartphone': 1530, 'smile': 830, 'smiley face': 2862, 'smiling': 1437, 'smoke': 593, 'smoking': 1636, 'smooth': 2097, 'smoothie': 2021, 'snake': 2720, 'sneakers': 689, 'sniffing': 3007, 'snow': 236, 'snowboard': 387, 'snowboarder': 1020, 'snowboarding': 234, 'snowboards': 1539, 'snowflakes': 2886, 'snowing': 1586, 'snowsuit': 2431, 'snowy': 2220, 'soap': 1633, 'soccer': 169, 'soccer ball': 1143, 'soccer field': 2468, 'socks': 167, 'soda': 637, 'sofa': 1824, 'soft': 1619, 'softball': 1012, 'soldier': 174, 'soldiers': 2169, 'solid': 94, 'someone': 702, 'sony': 1715, 'sony ericsson': 2548, 'soon': 3085, 'soup': 1294, 'south': 810, 'southwest': 291, 'space': 2217, 'space needle': 2916, 'space shuttle': 2611, 'spaghetti': 2174, 'spanish': 105, 'sparrow': 1345, 'spatula': 518, 'speaker': 2172, 'speakers': 1134, 'spectators': 274, 'speed limit': 2121, 'spices': 1536, 'spider': 2816, 'spiderman': 2397, 'spinach': 85, 'spiral': 2709, 'spoon': 408, 'spoons': 2391, 'sports': 1687, 'spots': 1034, 'spotted': 1997, 'spray paint': 1508, 'spring': 506, 'sprinkles': 685, 'sprint': 1573, 'sprite': 2470, 'square': 62, 'squares': 530, 'squash': 1994, 'squatting': 976, 'squirrel': 2632, \"st patrick's day\": 2443, 'stability': 2678, 'stadium': 2948, 'stagecoach': 2015, 'stained glass': 1924, 'stainless steel': 539, 'stairs': 127, 'stand': 1397, 'standing': 218, 'standing still': 1488, 'stands': 767, 'star': 220, 'star alliance': 2976, 'star wars': 3037, 'starbucks': 2291, 'staring': 1548, 'stars': 1485, 'state farm': 1705, 'station': 1053, 'statue': 1868, 'statues': 3042, 'steak': 632, 'steam': 1451, 'steamed': 712, 'steel': 918, 'steeple': 429, 'steering wheel': 667, 'steps': 2518, 'stew': 1770, 'stick': 630, 'sticker': 1988, 'stickers': 1195, 'sticks': 3043, 'still': 1340, 'stir fry': 2644, 'stomach': 2977, 'stone': 794, 'stones': 1933, 'stool': 1887, 'stop': 202, 'stop light': 2887, 'stop sign': 761, 'stopped': 2563, 'stopping': 999, 'storage': 1180, 'store': 533, 'stork': 2779, 'storm': 1914, 'stove': 1295, 'straight': 1092, 'straight ahead': 2867, 'strap': 327, 'straw': 1382, 'strawberries': 580, 'strawberry': 608, 'street': 268, 'street light': 325, 'street name': 2313, 'street sign': 1191, 'stretching': 2892, 'strike': 1871, 'string': 1542, 'stripe': 2057, 'striped': 301, 'stripes': 65, 'stroller': 857, 'stucco': 2093, 'student': 2346, 'students': 2362, 'stuffed': 1456, 'stuffed animal': 1590, 'stuffed animals': 859, 'style': 1986, 'styrofoam': 2029, 'sub': 866, 'subway': 1820, 'sugar': 741, 'suit': 715, 'suitcase': 584, 'suitcases': 2071, 'suits': 1750, 'summer': 570, 'sun': 143, 'sun hat': 1959, 'sunbathing': 1457, 'sunflower': 2138, 'sunflowers': 1570, 'sunglasses': 456, 'sunlight': 1105, 'sunny': 375, 'sunrise': 2964, 'sunset': 2083, 'supreme': 1175, 'surf': 1452, 'surfboard': 21, 'surfboards': 992, 'surfer': 576, 'surfers': 2825, 'surfing': 360, 'surprise': 1515, 'surprised': 1992, 'sushi': 2718, 'suspenders': 2909, 'suv': 1065, 'suzuki': 2968, 'swan': 1182, 'swans': 2436, 'sweat': 1200, 'sweatband': 1220, 'sweater': 270, 'sweatshirt': 2888, 'sweet': 117, 'sweet potato': 115, 'swim': 238, 'swim trunks': 1897, 'swimming': 878, 'swimsuit': 764, 'swing': 2740, 'swinging': 1370, 'swinging bat': 3015, 'swirls': 2447, 'swiss': 1961, 'switzerland': 2891, 'sydney': 1138, 'syrup': 349, 't': 1709, 't shirt': 501, 't shirt and jeans': 2150, 'tabby': 808, 'table': 81, 'tablecloth': 1364, 'tables': 1615, 'tablet': 1665, 'tag': 104, 'tags': 1000, 'tail': 2094, 'take off': 2843, 'taking off': 1273, 'taking photo': 3052, 'taking picture': 347, 'taking pictures': 2312, 'taking selfie': 1476, 'talking': 1004, 'talking on phone': 660, 'tall': 1021, 'taller': 2536, 'tam': 3101, 'tan': 77, 'tank': 485, 'tank top': 2344, 'tape': 344, 'target': 2722, 'tarmac': 2081, 'tarp': 870, 'tater tots': 2221, 'tattoo': 1270, 'tattoos': 2328, 'taxi': 1928, 'tea': 282, 'teacher': 2463, 'teal': 2078, 'team': 1692, 'teddy': 63, 'teddy bear': 582, 'teddy bears': 166, 'teeth': 2102, 'telephone': 2374, 'television': 1133, 'tell time': 2940, 'telling time': 1944, 'tennis': 257, 'tennis ball': 1471, 'tennis court': 645, 'tennis player': 2193, 'tennis racket': 208, 'tennis rackets': 254, 'tennis racquet': 1302, 'tennis shoes': 410, 'tent': 1335, 'tents': 2326, 'terrier': 1700, 'texas': 1503, 'texting': 131, 'thai': 1179, 'thailand': 1181, 'thanksgiving': 841, 'theater': 1301, \"they aren't\": 2324, 'thick': 86, 'thin': 1176, 'thomas': 2641, 'thoroughbred': 2268, 'thousands': 1972, 'throw': 1346, 'throw ball': 2285, 'throw frisbee': 2533, 'throwing': 736, 'throwing frisbee': 1635, 'thumb': 1124, 'thumbs up': 2190, 'tiara': 2361, 'tie': 207, 'tie dye': 2853, 'ties': 2273, 'tiger': 419, 'tigers': 2575, 'tile': 185, 'tiled': 2181, 'tiles': 890, 'tim hortons': 2750, 'time': 1328, 'tinkerbell': 2883, 'tire': 1777, 'tired': 936, 'tires': 2074, 'tissue': 495, 'tissues': 2127, 'to catch ball': 1644, 'to catch frisbee': 1571, 'to dry': 467, 'to eat': 1830, 'to get to other side': 2434, 'to hit ball': 2962, 'to left': 1538, 'to right': 1313, 'to see': 2516, 'toast': 1127, 'toasted': 1566, 'toaster': 515, 'toaster oven': 184, 'toilet': 851, 'toilet brush': 1149, 'toilet paper': 1836, 'toiletries': 2466, 'toilets': 2688, 'tokyo': 2576, 'tomato': 148, 'tomatoes': 1330, 'tongs': 1281, 'tongue': 1495, 'tools': 2711, 'toothbrush': 648, 'toothbrushes': 405, 'toothpaste': 2079, 'toothpick': 1447, 'toothpicks': 1901, 'top': 850, 'top hat': 2333, 'top left': 834, 'top right': 1210, 'toronto': 1839, 'toshiba': 589, 'tour': 2551, 'tourist': 2834, 'tow': 2110, 'tow truck': 2234, 'toward': 1169, 'towards': 981, 'towel': 93, 'towels': 1380, 'tower': 1403, 'towing': 2303, 'town': 294, 'toy': 1285, 'toyota': 409, 'toys': 2297, 'track': 2321, 'tracks': 221, 'tractor': 1670, 'traffic': 2047, 'traffic light': 1733, 'traffic lights': 2969, 'trailer': 555, 'train': 521, 'train car': 2676, 'train station': 928, 'train tracks': 847, 'trains': 1634, 'transport': 1204, 'transportation': 374, 'trash': 1697, 'trash can': 1100, 'travel': 721, 'traveling': 955, 'tray': 1441, 'tree': 35, 'tree branch': 2689, 'trees': 152, 'triangle': 590, 'triangles': 3039, 'trick': 1806, 'tripod': 2647, 'triumph': 2954, 'trolley': 888, 'tropical': 2222, 'tropicana': 2187, 'truck': 190, 'trucks': 2814, 'trunk': 1707, 'trunks': 2815, 'tub': 2167, 'tube': 1470, 'tugboat': 3053, 'tulip': 2979, 'tulips': 1332, 'tuna': 1565, 'tunnel': 1258, 'turkey': 825, 'turn': 476, 'turn right': 2902, 'turning': 1307, 'turtle': 453, 'tusks': 2634, 'tuxedo': 2725, 'tv': 425, 'tv stand': 2893, 'twin': 499, 'twins': 692, 'tying tie': 2672, 'typing': 916, 'uk': 2596, 'umbrella': 296, 'umbrellas': 802, 'umpire': 1202, 'unclear': 2388, 'under': 2542, 'under armour': 1940, 'under sink': 1963, 'under table': 2490, 'under tree': 2716, 'uniform': 1325, 'uniforms': 860, 'union station': 881, 'united': 1510, 'united states': 950, 'unknown': 545, 'unsure': 1521, 'up': 470, 'uphill': 1713, 'upright': 2460, 'ups': 3075, 'upside down': 2761, 'urban': 135, 'urinal': 2148, 'urinals': 2457, 'us': 2134, 'us air force': 3078, 'us airways': 3095, 'us airways express': 894, 'us open': 2159, 'usa': 292, 'used': 2773, 'using computer': 1912, 'using laptop': 1194, 'utensils': 2373, 'v': 2666, 'vacation': 1801, 'vaio': 2393, \"valentine's day\": 1671, 'van': 953, 'vanilla': 52, 'vans': 795, 'vase': 491, 'vases': 1680, 'vegetable': 1664, 'vegetables': 611, 'vegetarian': 782, 'veggie': 585, 'veggies': 3050, 'vehicles': 2602, 'venice': 2259, 'vent': 28, 'verizon': 1329, 'vertical': 303, 'very': 112, 'very big': 26, 'very deep': 763, 'very fast': 2437, 'very high': 693, 'very long': 2915, 'very old': 1724, 'very tall': 1103, 'vest': 1348, 'vests': 2342, 'victoria': 1804, 'victorian': 428, 'video': 3109, 'video game': 1145, 'vines': 2717, 'virgin': 1404, 'virgin atlantic': 3072, 'visibility': 1645, 'visilab': 3125, 'visor': 2010, 'volkswagen': 1373, 'volleyball': 746, 'volvo': 2664, 'w': 356, 'waffle': 759, 'wagon': 1164, 'waiting': 2441, 'wakeboard': 1591, 'walgreens': 3069, 'walk': 1507, 'walking': 157, 'wall': 78, 'wall st': 2351, 'wallet': 1564, 'wallpaper': 2139, 'war': 946, 'warm': 1245, 'warmth': 3012, 'warning': 2219, 'washing': 2251, 'washington': 972, 'washington dc': 1425, 'washington monument': 2927, 'watch': 345, 'watch tv': 2978, 'watching': 1559, 'watching tv': 919, 'water': 438, 'water bottle': 823, 'water ski': 2353, 'water skiing': 351, 'water skis': 551, 'watermark': 3016, 'watermelon': 249, 'wave': 1821, 'waves': 1097, 'waving': 2129, 'wavy': 2628, 'wax': 2619, 'wax paper': 2633, 'weather vane': 364, 'website': 1274, 'wedding': 479, 'weeds': 2882, 'welcome': 522, 'west': 2236, 'western': 2668, 'westin': 1640, 'westjet': 3076, 'wet': 1523, 'wetsuit': 762, 'wetsuits': 2849, 'whale': 490, 'wheat': 2208, 'wheel': 1422, 'wheelchair': 2331, 'wheelie': 2660, 'wheels': 1904, 'whipped cream': 3108, 'whirlpool': 3104, 'white': 4, 'white and black': 1594, 'white and blue': 365, 'white and brown': 671, 'white and gray': 942, 'white and green': 373, 'white and orange': 2112, 'white and pink': 1082, 'white and red': 1406, 'white and yellow': 867, 'white house': 2163, 'whole': 2042, 'wicker': 892, 'wide': 2648, 'wii': 33, 'wii controller': 1727, 'wii controllers': 2349, 'wii remote': 1306, 'wii remotes': 1064, 'wiimote': 2560, 'wild': 503, 'wildebeest': 2556, 'willow': 2334, 'wilson': 96, 'wind': 1297, 'windmill': 2311, 'window': 121, 'window sill': 502, 'windows': 817, 'windowsill': 1122, 'windsor': 2253, 'windsurfing': 510, 'windy': 1803, 'wine': 239, 'wine bottle': 2947, 'wine glass': 1465, 'wine glasses': 1815, 'wine tasting': 2541, 'wing': 3003, 'wings': 2514, 'winnie pooh': 2427, 'winter': 280, 'wire': 46, 'wireless': 2729, 'wires': 738, 'wisconsin': 3090, 'woman': 298, \"woman's\": 1991, 'women': 1019, \"women's\": 743, 'wood': 83, 'wooden': 700, 'woodpecker': 3124, 'woods': 1455, 'wool': 1572, 'words': 145, 'work': 1271, 'working': 1150, 'worms': 1696, 'wreath': 1312, 'wrist': 803, 'wristband': 1903, 'writing': 898, 'x': 1554, 'xbox': 1854, 'y': 2795, 'yacht': 1039, 'yamaha': 1104, 'yankees': 225, 'yard': 1745, 'yarn': 1469, 'years': 3048, 'yellow': 120, 'yellow and black': 458, 'yellow and blue': 271, 'yellow and green': 1810, 'yellow and orange': 2617, 'yellow and red': 517, 'yellow and white': 72, 'yes': 3, 'yield': 1885, 'yogurt': 1196, 'young': 698, 'younger': 1778, 'zebra': 395, 'zebra and giraffe': 2087, 'zebras': 171, 'zig zag': 2213, 'zipper': 2696, 'zoo': 575, 'zucchini': 1831}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ewewqKI2Bi2Y"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"/content/images.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyvRDOowphW9"
      },
      "source": [
        "# Create label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcqJO6epkaS4",
        "outputId": "66c7bdeb-0047-4410-c30f-e9253f7e212a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Pink Block', 1: 'Purple Block', 2: 'Blue Block', 3: 'Gray Block', 4: 'Cyan Block', 5: 'Green Block'}\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def create_dictionary_from_csv(csv_file_path):\n",
        "    result_dict = {}\n",
        "    label2id_dict= {}\n",
        "    with open('/content/updated_output.csv', 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            label = int(row['label'])\n",
        "            answer = row['answer']\n",
        "            result_dict[label] = answer\n",
        "            label2id_dict[answer] = label\n",
        "    return result_dict\n",
        "\n",
        "def label2id(csv_file_path):\n",
        "    result_dict = {}\n",
        "    label2id_dict= {}\n",
        "    with open('/content/updated_output.csv', 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            label = int(row['label'])\n",
        "            answer = row['answer']\n",
        "            result_dict[label] = answer\n",
        "            label2id_dict[answer] = label\n",
        "    return label2id_dict\n",
        "\n",
        "# Provide the path to your CSV file\n",
        "csv_file_path = '/content'\n",
        "\n",
        "# Call the function to create the dictionary\n",
        "dictionary = create_dictionary_from_csv(csv_file_path)\n",
        "label2id_dict = label2id(csv_file_path)\n",
        "id2label_dict = dictionary\n",
        "# Print the resulting dictionary\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_unique_labels(csv_file):\n",
        "    try:\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Delete the existing \"label\" column if it exists\n",
        "        if 'label' in df.columns:\n",
        "            df.drop(columns=['label'], inplace=True)\n",
        "\n",
        "        # Create a mapping dictionary to assign unique numerical labels to each unique \"answer\" value\n",
        "        unique_answers = df['answer'].unique()\n",
        "        label_mapping = {answer: label for label, answer in enumerate(unique_answers)}\n",
        "\n",
        "        # Create the new \"label\" column using the mapping\n",
        "        df['label'] = df['answer'].map(label_mapping)\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(\"updated_output.csv\", index=False)\n",
        "\n",
        "        print(\"New 'label' column with unique numerical values has been created and saved as updated_output.csv.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file_path = \"/content/sorted_output.csv\"\n",
        "    create_unique_labels(csv_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHW-RVARVjJK",
        "outputId": "f3e2693c-9402-4533-ecd0-f07ee0ea4014"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New 'label' column with unique numerical values has been created and saved as updated_output.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data class"
      ],
      "metadata": {
        "id": "SgnFZyymuDXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "FJpb8rhJRX4h"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "kJ3sm7qGAyab"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "df = pd.read_csv('/content/updated_output.csv')\n",
        "questions = df['question']\n",
        "#print(questions)\n",
        "annotations = df['answer']\n",
        "#print(annotations)\n",
        "labels = df['label']\n",
        "#print(labels)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    \"\"\"VQA (v2) dataset.\"\"\"\n",
        "    def __init__(self, df, processor, tokenizer):\n",
        "    # def __init__(self, questions, annotations, processor):\n",
        "        self.df = df\n",
        "        self.questions = df['question']\n",
        "        print(self.questions)\n",
        "        self.annotations = df['answer']\n",
        "        print(self.annotations)\n",
        "        self.labels = df['label']\n",
        "        print(self.labels)\n",
        "        self.processor = processor\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get image + text\n",
        "        annotation = self.annotations[idx]\n",
        "        questions = self.questions[idx]\n",
        "        img_id = self.df['image'][idx]\n",
        "        #print(img_id)\n",
        "        image = Image.open(f\"images/{img_id}.jpg\")\n",
        "        labels = self.labels[idx]\n",
        "        #image.resize((128, 128))\n",
        "        text = questions\n",
        "        targets = torch.zeros(len(id2label_dict))\n",
        "        targets[labels-1] = 1\n",
        "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        # remove batch dimension\n",
        "        for k,v in encoding.items():\n",
        "          # print(k)\n",
        "          encoding[k] = v.squeeze()\n",
        "        #targets = torch.zeros(len(id2label_dict))\n",
        "        tokens = tokenizer.tokenize(annotation)\n",
        "        #tokens = [int(token) for token in tokens]\n",
        "        #print(tokens)\n",
        "\n",
        "        encoding[\"labels\"] = targets\n",
        "        #encoding[\"labels\"] = torch.tensor(encoding[\"labels\"])\n",
        "        #encoding[\"labels\"] = encoding[\"labels\"].view(3129)\n",
        "        #print(tokens)\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = torch.zeros(len(config.id2label))\n",
        "targets.shape"
      ],
      "metadata": {
        "id": "7kSmmOjAVVGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45bf0d1-5022-41ea-adaa-2d1c44a7d351"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3129])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltForQuestionAnswering\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",\n",
        "                                                 num_labels=len(id2label_dict),\n",
        "                                                 id2label=id2label_dict,\n",
        "                                                 label2id=label2id_dict)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XibMM4tx5kj",
        "outputId": "758feca2-a449-4ecc-8f5c-ed60289e7cdf"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.0.bias', 'classifier.3.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViltForQuestionAnswering(\n",
              "  (vilt): ViltModel(\n",
              "    (embeddings): ViltEmbeddings(\n",
              "      (text_embeddings): TextEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768)\n",
              "        (position_embeddings): Embedding(40, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (patch_embeddings): ViltPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "      )\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViltEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViltLayer(\n",
              "          (attention): ViltAttention(\n",
              "            (attention): ViltSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViltSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViltIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViltOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViltPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
              "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Linear(in_features=1536, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Dl2UsPrTHbtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630a809e-118d-4eac-e663-7b3579cc3bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                  Which object is closer to the middle?\n",
            "1                  Which object is closer to the middle?\n",
            "2                  Which object is closer to the middle?\n",
            "3                  Which object is closer to the middle?\n",
            "4               Which object is closest to the red bowl?\n",
            "                             ...                        \n",
            "419    Which block is farther from the middle, the pu...\n",
            "420            Which object is closest to the blue bowl?\n",
            "421        Which block is farther from the orange block?\n",
            "422            Which object is closest to the blue bowl?\n",
            "423              Which block is farther from the middle?\n",
            "Name: question, Length: 424, dtype: object\n",
            "0        Pink Block\n",
            "1        Pink Block\n",
            "2        Pink Block\n",
            "3        Pink Block\n",
            "4      Purple Block\n",
            "           ...     \n",
            "419    Purple Block\n",
            "420      Pink Block\n",
            "421      Pink Block\n",
            "422      Pink Block\n",
            "423    Purple Block\n",
            "Name: answer, Length: 424, dtype: object\n",
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      1\n",
            "      ..\n",
            "419    1\n",
            "420    0\n",
            "421    0\n",
            "422    0\n",
            "423    1\n",
            "Name: label, Length: 424, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViltProcessor\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "\n",
        "dataset = VQADataset(df=df,\n",
        "                     processor=processor, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "FBcmLQ6AJb4M",
        "outputId": "42cbff2c-fc50-4d26-e414-37bc9c4b1874"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] which object is closer to the middle? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "processor.decode(dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "AGxYDVSve-O6"
      },
      "outputs": [],
      "source": [
        "labels = torch.nonzero(dataset[0]['labels']).squeeze().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tThbn-ilfmYi"
      },
      "source": [
        "## Define model\n",
        "\n",
        "Here we define a `ViltForQuestionAnswering` model, with the weights of the body initialized from dandelin/vilt-b32-mlm, and a randomly initialized classification head. We also move it to the GPU, if it's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "9C4gCxrbgjqh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "  input_ids = [item['input_ids'] for item in batch]\n",
        "  pixel_values = [item['pixel_values'] for item in batch]\n",
        "  attention_mask = [item['attention_mask'] for item in batch]\n",
        "  token_type_ids = [item['token_type_ids'] for item in batch]\n",
        "  labels = [item['labels'] for item in batch]\n",
        "\n",
        "  # create padded pixel values and corresponding pixel mask\n",
        "  encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
        "\n",
        "  # create new batch\n",
        "  batch = {}\n",
        "  batch['input_ids'] = torch.stack(input_ids)\n",
        "  batch['attention_mask'] = torch.stack(attention_mask)\n",
        "  batch['token_type_ids'] = torch.stack(token_type_ids)\n",
        "  batch['pixel_values'] = encoding['pixel_values']\n",
        "  batch['pixel_mask'] = encoding['pixel_mask']\n",
        "  # batch['pad_and_create_pixel_mask'] =\n",
        "  batch['labels'] = torch.stack(labels)\n",
        "\n",
        "  return batch\n",
        "\n",
        "# train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
        "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "yKBOBxq1gqzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26951fe2-e162-4d83-92b9-4f9e2a22e109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'training': False, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('vilt', ViltModel(\n",
            "  (embeddings): ViltEmbeddings(\n",
            "    (text_embeddings): TextEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768)\n",
            "      (position_embeddings): Embedding(40, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (patch_embeddings): ViltPatchEmbeddings(\n",
            "      (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
            "    )\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (encoder): ViltEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x ViltLayer(\n",
            "        (attention): ViltAttention(\n",
            "          (attention): ViltSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (output): ViltSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): ViltIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): ViltOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (pooler): ViltPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")), ('classifier', Sequential(\n",
            "  (0): Linear(in_features=768, out_features=1536, bias=True)\n",
            "  (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  (2): GELU(approximate='none')\n",
            "  (3): Linear(in_features=1536, out_features=6, bias=True)\n",
            "))]), 'config': ViltConfig {\n",
            "  \"_name_or_path\": \"dandelin/vilt-b32-mlm\",\n",
            "  \"architectures\": [\n",
            "    \"ViltForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"Pink Block\",\n",
            "    \"1\": \"Purple Block\",\n",
            "    \"2\": \"Blue Block\",\n",
            "    \"3\": \"Gray Block\",\n",
            "    \"4\": \"Cyan Block\",\n",
            "    \"5\": \"Green Block\"\n",
            "  },\n",
            "  \"image_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"Blue Block\": 2,\n",
            "    \"Cyan Block\": 4,\n",
            "    \"Gray Block\": 3,\n",
            "    \"Green Block\": 5,\n",
            "    \"Pink Block\": 0,\n",
            "    \"Purple Block\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_image_length\": -1,\n",
            "  \"max_position_embeddings\": 40,\n",
            "  \"modality_type_vocab_size\": 2,\n",
            "  \"model_type\": \"vilt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_images\": -1,\n",
            "  \"patch_size\": 32,\n",
            "  \"qkv_bias\": true,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            ", 'name_or_path': 'dandelin/vilt-b32-mlm', 'warnings_issued': {}, 'generation_config': None, 'num_labels': 6, '_is_hf_initialized': True, 'is_loaded_in_4bit': False, 'is_loaded_in_8bit': False, 'is_quantized': False}\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(vars(model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1Y_i-Y15qBB",
        "outputId": "c6674df1-2fe7-462a-eaf9-8e2b8d648ba1"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZkar-taiq1r",
        "outputId": "3f1a873b-23ec-416a-e05c-72bacf64d50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids torch.Size([4, 40])\n",
            "token_type_ids torch.Size([4, 40])\n",
            "attention_mask torch.Size([4, 40])\n",
            "pixel_values torch.Size([4, 3, 384, 384])\n",
            "pixel_mask torch.Size([4, 384, 384])\n",
            "labels torch.Size([4, 6])\n"
          ]
        }
      ],
      "source": [
        "for k,v in batch.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Set the optimizer and learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Set the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the total loss for this epoch\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate over the batches in the train_dataloader\n",
        "    for batch in train_dataloader:\n",
        "        # Move the batch to the device\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "        # Clear out the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(outputs.logits, batch['labels'])\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Training is complete\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jen0fB06aa3P",
        "outputId": "8f7729c8-f463-4331-e11a-cee67db5e1e6"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Average Loss: 1.5777\n",
            "Epoch 2/10, Average Loss: 1.4051\n",
            "Epoch 3/10, Average Loss: 1.2044\n",
            "Epoch 4/10, Average Loss: 0.9747\n",
            "Epoch 5/10, Average Loss: 0.7210\n",
            "Epoch 6/10, Average Loss: 0.5099\n",
            "Epoch 7/10, Average Loss: 0.4138\n",
            "Epoch 8/10, Average Loss: 0.3027\n",
            "Epoch 9/10, Average Loss: 0.2094\n",
            "Epoch 10/10, Average Loss: 0.1594\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BCEloss"
      ],
      "metadata": {
        "id": "5uEI180e4-_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "# Set the optimizer and learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Set the loss function\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the total loss for this epoch\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate over the batches in the train_dataloader\n",
        "    for batch in train_dataloader:\n",
        "        # Move the batch to the device\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "        # Clear out the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**batch)\n",
        "        m = torch.nn.Softmax()\n",
        "        # print(outputs.logits.size(), m(outputs.logits).size())\n",
        "        #print(torch.max(m(outputs.logits), 1))\n",
        "        # print(torch.std_mean(m(outputs.logits)))\n",
        "        # print(outputs.logits)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(outputs.logits, batch['labels'])\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Training is complete\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09fSRW8PFcPq",
        "outputId": "de949385-6b20-49a7-f396-875669b0c120"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Average Loss: 0.2250\n",
            "Epoch 2/10, Average Loss: 0.2062\n",
            "Epoch 3/10, Average Loss: 0.1910\n",
            "Epoch 4/10, Average Loss: 0.1756\n",
            "Epoch 5/10, Average Loss: 0.1600\n",
            "Epoch 6/10, Average Loss: 0.1456\n",
            "Epoch 7/10, Average Loss: 0.1281\n",
            "Epoch 8/10, Average Loss: 0.1170\n",
            "Epoch 9/10, Average Loss: 0.0955\n",
            "Epoch 10/10, Average Loss: 0.0901\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calling_vilt_model(image, question, device):\n",
        "    \"\"\"\n",
        "    Calling the ViLT model\n",
        "    \"\"\"\n",
        "    test_encoding = processor(image, question, return_tensors=\"pt\")\n",
        "    test_encoding = {k: v.to(device) for k,v in test_encoding.items()}\n",
        "    test_logits = model(**test_encoding).logits\n",
        "    m = torch.nn.Sigmoid()\n",
        "    # print(m(logits))\n",
        "    # print(f\"Standard deviation in confidence {torch.std_mean(m(logits)).cpu().numpy()}\" )\n",
        "    print(torch.max(m(test_logits)))\n",
        "    print((m(test_logits)))\n",
        "    answer = model.config.id2label[test_logits.argmax(-1).item()+1]\n",
        "    print(test_logits.argmax(-1))\n",
        "    print(torch.max(m(test_logits)))\n",
        "    print((m(test_logits)))\n",
        "    print(\"\\n\\033[1;31;34m>> Predicted answer =\", answer)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "TAeku030WFy9"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = \"Which object is closest to the brown bowl, the blue block or the orange block?\"\n",
        "example_image = Image.open(\"/content/img5.jpg\")\n",
        "answer = calling_vilt_model(example_image, questions, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d-pxFEiWHrJ",
        "outputId": "d68ea75e-4cd2-45b3-8603-530287df1ab8"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "tensor([[0.5526, 0.9950, 0.6070, 0.0385, 0.0401, 0.3740]], device='cuda:0',\n",
            "       grad_fn=<SigmoidBackward0>)\n",
            "tensor([1], device='cuda:0')\n",
            "tensor(0.9950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "tensor([[0.5526, 0.9950, 0.6070, 0.0385, 0.0401, 0.3740]], device='cuda:0',\n",
            "       grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "\u001b[1;31;34m>> Predicted answer = Blue Block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR4c68kepubw"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oIHYdCITyam",
        "outputId": "6c3113ee-bcb4-4e00-f287-5ba4c46ecba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fdace676410>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "57PlqyxXf6L4",
        "outputId": "0c9301e3-6f90-46c7-c79a-65aa7ddc8afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-b7fb385c2278>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Transpose the batch to have a list of tensors for each input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Pad the sequences to a uniform length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-b7fb385c2278>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Transpose the batch to have a list of tensors for each input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Pad the sequences to a uniform length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm as T\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Assuming train_dataloader contains your training data\n",
        "\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        # Transpose the batch to have a list of tensors for each input\n",
        "        inputs = [torch.tensor(item).to(device) for item in zip(*batch)]\n",
        "\n",
        "        # Pad the sequences to a uniform length\n",
        "        inputs = pad_sequence(inputs, batch_first=True)\n",
        "\n",
        "        # Get the input tensors from the padded batch\n",
        "        input_ids, attention_masks, labels = inputs\n",
        "\n",
        "        # Rest of your training code goes here...\n",
        "\n",
        "        # get the inputs;\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        print(\"Loss:\", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nyeUUMWpz6a"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calling_vilt_model(image, question, device):\n",
        "    \"\"\"\n",
        "    Calling the ViLT model\n",
        "    \"\"\"\n",
        "    encoding = processor(image, question, return_tensors=\"pt\")\n",
        "    encoding = {k: v.to(device) for k,v in encoding.items()}\n",
        "    logits = model(**encoding).logits\n",
        "    m = torch.nn.Sigmoid()\n",
        "    print(m(logits))\n",
        "    print(torch.std_mean(m(logits)))\n",
        "    answer = model.config.id2label[logits.argmax(-1).item()]\n",
        "\n",
        "    print(\"\\n\\033[1;31;34m>> Predicted answer =\", answer)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "MO2yRg6BbW-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z70Di08I-QGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72eb1cfe-4a9c-4d78-edcc-80582cd48596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0018, 0.0026, 0.0026, 0.0021, 0.0020, 0.0028, 0.0024, 0.0023, 0.0019,\n",
            "         0.0024, 0.0018, 0.0020, 0.0027, 0.0017, 0.0015, 0.0011, 0.0013, 0.0026,\n",
            "         0.0023, 0.0028, 0.0020, 0.0022, 0.0019, 0.0020, 0.0033, 0.0010, 0.0022,\n",
            "         0.0026, 0.0026, 0.0028, 0.0024, 0.0014, 0.0016, 0.0016, 0.0036, 0.0030,\n",
            "         0.0024, 0.0017, 0.0024, 0.0021, 0.0020, 0.0019, 0.0022, 0.0013, 0.0031,\n",
            "         0.0019, 0.0018, 0.0028, 0.0023, 0.0022, 0.0024, 0.0024, 0.0017, 0.0012,\n",
            "         0.0018, 0.0015, 0.0032, 0.0018, 0.0025, 0.0016, 0.0028, 0.0020, 0.0022,\n",
            "         0.0019, 0.0034, 0.0030, 0.0016, 0.0019, 0.0017, 0.0023, 0.0024, 0.0022,\n",
            "         0.0025, 0.0018, 0.0025, 0.0023, 0.0019, 0.0028, 0.0017, 0.0023, 0.0023,\n",
            "         0.0026, 0.0016, 0.0019, 0.0019, 0.0019, 0.0029, 0.0026, 0.0022, 0.0022,\n",
            "         0.0020, 0.0018, 0.0015, 0.0025, 0.0021, 0.0014, 0.0018, 0.0016, 0.0018,\n",
            "         0.0028, 0.0021, 0.0016, 0.0026, 0.0024, 0.0021, 0.0017, 0.0023, 0.0021,\n",
            "         0.0018, 0.0022, 0.0019, 0.0013, 0.0030, 0.0026, 0.0032, 0.0020, 0.0022,\n",
            "         0.0022, 0.0018, 0.0025, 0.0016, 0.0018, 0.0015, 0.0018, 0.0021, 0.0024,\n",
            "         0.0020, 0.0027, 0.0023, 0.0019, 0.0017, 0.0019, 0.0019, 0.0023, 0.0021,\n",
            "         0.0019, 0.0017, 0.0019, 0.0023, 0.0025, 0.0015, 0.0018, 0.0024, 0.0021,\n",
            "         0.0015, 0.0015, 0.0021, 0.0018, 0.0019, 0.0026, 0.0025, 0.0025, 0.0019]],\n",
            "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
            "(tensor(0.0005, device='cuda:0', grad_fn=<StdMeanBackward0>), tensor(0.0021, device='cuda:0', grad_fn=<StdMeanBackward0>))\n",
            "\n",
            "\u001b[1;31;34m>> Predicted answer = This image contains three blocks, one gray, one purple, and one orange, as well as two bowls, one gray and one pink.\n"
          ]
        }
      ],
      "source": [
        "questions = \"Which object is closer to the middle, the yellow block or the gray block?\"\n",
        "example_image = Image.open(\"/content/img4.jpg\")\n",
        "answer = calling_vilt_model(example_image, questions, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS_ixA8KDOfg"
      },
      "outputs": [],
      "source": [
        "example = examples\n",
        "# add batch dimension + move to GPU\n",
        "example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n",
        "\n",
        "# forward pass\n",
        "result = model(**example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItZXKkxaEamq",
        "outputId": "a7fe7efb-4fbb-4d97-bb19-afe960378966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 153])\n",
            "tensor([[0.0043, 0.0063, 0.0063, 0.0062, 0.0067, 0.0063, 0.0055, 0.0078, 0.0045,\n",
            "         0.0062, 0.0071, 0.0063, 0.0081, 0.0067, 0.0063, 0.0074, 0.0092, 0.0048,\n",
            "         0.0034, 0.0102, 0.0058, 0.0076, 0.0059, 0.0052, 0.0095, 0.0050, 0.0059,\n",
            "         0.0063, 0.0052, 0.0103, 0.0066, 0.0094, 0.0077, 0.0043, 0.0065, 0.0043,\n",
            "         0.0037, 0.0068, 0.0058, 0.0069, 0.0038, 0.0065, 0.0064, 0.0064, 0.0069,\n",
            "         0.0067, 0.0036, 0.0049, 0.0080, 0.0078, 0.0040, 0.0065, 0.0061, 0.0056,\n",
            "         0.0082, 0.0071, 0.0060, 0.0042, 0.0082, 0.0064, 0.0048, 0.0078, 0.0051,\n",
            "         0.0062, 0.0057, 0.0075, 0.0058, 0.0047, 0.0035, 0.0072, 0.0066, 0.0042,\n",
            "         0.0076, 0.0054, 0.0070, 0.0062, 0.0072, 0.0066, 0.0052, 0.0059, 0.0068,\n",
            "         0.0055, 0.0070, 0.0076, 0.0103, 0.0060, 0.0062, 0.0070, 0.0059, 0.0074,\n",
            "         0.0047, 0.0060, 0.0058, 0.0059, 0.0066, 0.0070, 0.0074, 0.0034, 0.0055,\n",
            "         0.0078, 0.0062, 0.0056, 0.0085, 0.0082, 0.0079, 0.0055, 0.0068, 0.0059,\n",
            "         0.0073, 0.0057, 0.0089, 0.0065, 0.0054, 0.0045, 0.0070, 0.0071, 0.0079,\n",
            "         0.0052, 0.0063, 0.0077, 0.0058, 0.0063, 0.0071, 0.0050, 0.0075, 0.0081,\n",
            "         0.0062, 0.0069, 0.0054, 0.0075, 0.0064, 0.0067, 0.0098, 0.0054, 0.0109,\n",
            "         0.0067, 0.0074, 0.0090, 0.0067, 0.0087, 0.0100, 0.0072, 0.0060, 0.0074,\n",
            "         0.0055, 0.0046, 0.0093, 0.0054, 0.0072, 0.0078, 0.0086, 0.0082, 0.0032]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "(tensor(0.0015, device='cuda:0', grad_fn=<StdMeanBackward0>), tensor(0.0065, device='cuda:0', grad_fn=<StdMeanBackward0>))\n",
            "Predicted label: 134 Predicted answer: This environment contains three blocks - one purple, one green, and one orange - as well as two bowls - one gray and one orange.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-46d14317492d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  print(m(result_logits))\n",
            "<ipython-input-97-46d14317492d>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  print(torch.std_mean(m(result_logits)))\n"
          ]
        }
      ],
      "source": [
        "result_logits = result.logits\n",
        "print(torch.sigmoid(result_logits).size())\n",
        "predicted_class = result_logits.argmax(-1).item()\n",
        "m = torch.nn.Softmax()\n",
        "print(m(result_logits))\n",
        "print(torch.std_mean(m(result_logits)))\n",
        "print(f\"Predicted label: {predicted_class} Predicted answer: {model.config.id2label[predicted_class]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii99wHusp52Y"
      },
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNYPtPgyqYKF"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7zWXiA1ofwr"
      },
      "outputs": [],
      "source": [
        "text_content = \"\"\n",
        "\n",
        "image_descriptions = text_content\n",
        "image_blocks = image_descriptions.split('\\n\\n')\n",
        "output = \"\"\n",
        "\n",
        "for image_block in image_blocks:\n",
        "    image_name_start = image_block.index('Image Name: ')\n",
        "    #print(image_name_start)\n",
        "    image_name_end = image_block.index('.jpg')\n",
        "    image_name = image_block[image_name_start:image_name_end]\n",
        "    print(image_name)\n",
        "\n",
        "    caption_start = image_block.index('Caption: ')\n",
        "    caption = image_block[caption_start + len('Caption: '):].strip()\n",
        "    print(f\"Caption: {caption}\")\n",
        "\n",
        "    prompt = f\"\"\"ask five unique questions from this description: the questions should be strictly about two things. nothing else. These two things are:\n",
        "    1. which object is where (for example: which object is closer to the middle? or top or which block farther from middle?),\n",
        "    2. their relative distance (for example: which object is closest to the yellow bowl?).\n",
        "\n",
        "    Don't ask questions that:\n",
        "     1. may have mathematical values. For example, don't ask \"how far is the yellow block from the red block?\".\n",
        "     2. which object is higher or lower. Assume all blocks and bowls are on a tabletop environment. They are all on a level surface.\n",
        "\n",
        "    Ask the question for each captions: {caption}\n",
        "    \"\"\"\n",
        "    response = get_completion(prompt)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.randn(7)\n",
        "print(a)\n",
        "print(a.argmax(-1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1vWlYaK1y42",
        "outputId": "ccc8b391-91e6-4bce-a9bb-babb683b102f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.1771,  0.4629, -0.5983, -0.4261, -0.9190, -1.6253, -0.4263])\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def check_images_exist(csv_file):\n",
        "    try:\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Create a list of expected image values from \"img1\" to \"img107\"\n",
        "        expected_images = [f\"img{i}\" for i in range(1, 108)]\n",
        "\n",
        "        # Get the unique values from the \"image\" column in the DataFrame\n",
        "        unique_images = df['image'].unique()\n",
        "\n",
        "        # Check if all expected image values exist in the \"image\" column\n",
        "        missing_images = [image for image in expected_images if image not in unique_images]\n",
        "\n",
        "        if not missing_images:\n",
        "            print(\"All expected image values from 'img1' to 'img107' exist in the 'image' column.\")\n",
        "        else:\n",
        "            print(f\"Missing image values in the 'image' column: {', '.join(missing_images)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file_path = \"/content/sorted_output.csv\"\n",
        "    check_images_exist(csv_file_path)"
      ],
      "metadata": {
        "id": "Ec3A_Wf814z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec73fc1-7da5-4459-903f-6ccef9add36d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing image values in the 'image' column: img67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tARzdSBFT51J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6df866f35e9545e49e621d8785ab9ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a11bb38dcf424624bb76960ddb300d4d",
              "IPY_MODEL_44a077e053e34f3e8a47d15c53699d0d",
              "IPY_MODEL_ed5a46792ccb41d890363b4829df0353"
            ],
            "layout": "IPY_MODEL_7cdb8fbc1092466ea35e6860eb49f189"
          }
        },
        "a11bb38dcf424624bb76960ddb300d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64d95859e234c609f9257bd3c29d120",
            "placeholder": "​",
            "style": "IPY_MODEL_5a2da52c76bf4244a8d78bce58036621",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "44a077e053e34f3e8a47d15c53699d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a326818b45864539a980002e64bd2b37",
            "max": 135543,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ceafacf167064863a507c6b77dbb7fc6",
            "value": 135543
          }
        },
        "ed5a46792ccb41d890363b4829df0353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc930076859f4854bdd4041fb84794a5",
            "placeholder": "​",
            "style": "IPY_MODEL_b5fe2341b6db4f24b7595ab2f8bafe67",
            "value": " 136k/136k [00:00&lt;00:00, 2.29MB/s]"
          }
        },
        "7cdb8fbc1092466ea35e6860eb49f189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a64d95859e234c609f9257bd3c29d120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2da52c76bf4244a8d78bce58036621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a326818b45864539a980002e64bd2b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceafacf167064863a507c6b77dbb7fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc930076859f4854bdd4041fb84794a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5fe2341b6db4f24b7595ab2f8bafe67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}